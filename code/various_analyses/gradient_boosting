library(gbm)
library(h2o)
library(xgboost)
library(recipes)

set.seed(123)
index_1 <- createDataPartition(data_tree$job_satisfaction, p = 0.7,
                               list = FALSE)

train_1 <- data_tree[index_1, ]
test_1 <- data_tree[-index_1, ]

xgb_prep <- recipe(job_satisfaction ~., data = train_1) %>% 
  step_integer(all_nominal()) %>% 
  prep(training = train_1, retain = TRUE) %>% 
  juice()

X <- as.matrix(xgb_prep[setdiff(names(xgb_prep), "job_satisfaction")])
Y <- xgb_prep$job_satisfaction

set.seed(123)
xgb_partial <- xgb.cv(
  data = X,
  label = Y,
  nrounds = 6000,
  objetive = "reg:linear",
  early_stopping_rounds = 50,
  nfold = 10,
  params = list(
    eta = 0.1,
    max_depth = 3,
    min_child_weight = 3,
    subsample = 0.8,
    colsample_bytree = 1.0),
  verbose = 0
)

min(xgb_partial$evaluation_log$test_rmse_mean)

#hyperparameter grid to test overfitting

hyper_grid <- expand.grid(
  eta = 0.01,
  max_depth = 3, 
  min_child_weight = 3,
  subsample = 0.5, 
  colsample_bytree = 0.5,
  gamma = c(0, 1, 10, 100, 1000),
  lambda = c(0, 1e-2, 0.1, 1, 100, 1000, 10000),
  alpha = c(0, 1e-2, 0.1, 1, 100, 1000, 10000),
  rmse = 0,          
  trees = 0          
)



for(i in seq_len(nrow(hyper_grid))) {
  set.seed(123)
  m <- xgb.cv(
    data = X,
    label = Y,
    nrounds = 4000,
    objective = "reg:linear",
    early_stopping_rounds = 50, 
    nfold = 10,
    verbose = 0,
    params = list( 
      eta = hyper_grid$eta[i], 
      max_depth = hyper_grid$max_depth[i],
      min_child_weight = hyper_grid$min_child_weight[i],
      subsample = hyper_grid$subsample[i],
      colsample_bytree = hyper_grid$colsample_bytree[i],
      gamma = hyper_grid$gamma[i], 
      lambda = hyper_grid$lambda[i], 
      alpha = hyper_grid$alpha[i]
    ) 
  )
  hyper_grid$rmse[i] <- min(m$evaluation_log$test_rmse_mean)
  hyper_grid$trees[i] <- m$best_iteration
}

hyper_grid %>%
  filter(rmse > 0) %>%
  arrange(rmse) %>%
  glimpse()

params <- list(
  eta = 0.01,
  max_depth = 3,
  min_child_weight = 3,
  subsample = 0.5,
  colsample_bytree = 0.5
)

xgb.fit.final <- xgboost(
  params = params,
  data = X,
  label = Y,
  nrounds = 4000,
  objective = "reg:linear",
  verbose = 0
)

vip::vip(xgb.fit.final, num_features = 36) 


###Gradient Boosting - full.data



xgb_prep_full <- recipe(job_satisfaction ~., data = train_1_full) %>% 
  step_integer(all_nominal()) %>% 
  prep(training = train_1_full, retain = TRUE) %>% 
  juice()

A <- as.matrix(xgb_prep_full[setdiff(names(xgb_prep_full), "job_satisfaction")])
B <- xgb_prep_full$job_satisfaction


set.seed(123)
xgb_full <- xgb.cv(
  data = A,
  label = B,
  nrounds = 6000,
  objetive = "reg:linear",
  early_stopping_rounds = 50,
  nfold = 10,
  params = list(
    eta = 0.1,
    max_depth = 3,
    min_child_weight = 3,
    subsample = 0.8,
    colsample_bytree = 1.0),
  verbose = 0
)


min(xgb_full$evaluation_log$test_rmse_mean)

#hyperparameter grid to test overfitting

hyper_grid_full <- expand.grid(
  eta = 0.01,
  max_depth = 3, 
  min_child_weight = 3,
  subsample = 0.5, 
  colsample_bytree = 0.5,
  gamma = c(0, 1, 10, 100, 1000),
  lambda = c(0, 1e-2, 0.1, 1, 100, 1000, 10000),
  alpha = c(0, 1e-2, 0.1, 1, 100, 1000, 10000),
  rmse = 0,          
  trees = 0          
)

for(i in seq_len(nrow(hyper_grid_full))) {
  set.seed(123)
  m <- xgb.cv(
    data = A,
    label = B,
    nrounds = 4000,
    objective = "reg:linear",
    early_stopping_rounds = 50, 
    nfold = 10,
    verbose = 0,
    params = list( 
      eta = hyper_grid$eta[i], 
      max_depth = hyper_grid$max_depth[i],
      min_child_weight = hyper_grid$min_child_weight[i],
      subsample = hyper_grid$subsample[i],
      colsample_bytree = hyper_grid$colsample_bytree[i],
      gamma = hyper_grid$gamma[i], 
      lambda = hyper_grid$lambda[i], 
      alpha = hyper_grid$alpha[i]
    ) 
  )
  hyper_grid$rmse[i] <- min(m$evaluation_log$test_rmse_mean)
  hyper_grid$trees[i] <- m$best_iteration
}

hyper_grid_full %>%
  filter(rmse > 0) %>%
  arrange(rmse) %>%
  glimpse()
